{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 20 뉴스 그룹 분류"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": []
  },
  {
   "source": [
    "## 데이터 다운로드 (fetch)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups(subset='all', random_state=156)"
   ]
  },
  {
   "source": [
    "## 데이터 탐색"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "target 클래스의 값과 분포도\n0     799\n1     973\n2     985\n3     982\n4     963\n5     988\n6     975\n7     990\n8     996\n9     994\n10    999\n11    991\n12    984\n13    990\n14    987\n15    997\n16    910\n17    940\n18    775\n19    628\ndtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('target 클래스의 값과 분포도')\n",
    "print(pd.Series(dataset.target).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "target 클래스의 이름들 \n ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print('target 클래스의 이름들 \\n', dataset.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "From: egreen@east.sun.com (Ed Green - Pixel Cruncher)\nSubject: Re: Observation re: helmets\nOrganization: Sun Microsystems, RTP, NC\nLines: 21\nDistribution: world\nReply-To: egreen@east.sun.com\nNNTP-Posting-Host: laser.east.sun.com\n\nIn article 211353@mavenry.altcit.eskimo.com, maven@mavenry.altcit.eskimo.com (Norman Hamer) writes:\n> \n> The question for the day is re: passenger helmets, if you don't know for \n>certain who's gonna ride with you (like say you meet them at a .... church \n>meeting, yeah, that's the ticket)... What are some guidelines? Should I just \n>pick up another shoei in my size to have a backup helmet (XL), or should I \n>maybe get an inexpensive one of a smaller size to accomodate my likely \n>passenger? \n\nIf your primary concern is protecting the passenger in the event of a\ncrash, have him or her fitted for a helmet that is their size.  If your\nprimary concern is complying with stupid helmet laws, carry a real big\nspare (you can put a big or small head in a big helmet, but not in a\nsmall one).\n\n---\nEd Green, former Ninjaite |I was drinking last night with a biker,\n  Ed.Green@East.Sun.COM   |and I showed him a picture of you.  I said,\nDoD #0111  (919)460-8302  |\"Go on, get to know her, you'll like her!\"\n (The Grateful Dead) -->  |It seemed like the least I could do...\n\n\n"
     ]
    }
   ],
   "source": [
    "print(dataset.data[0])"
   ]
  },
  {
   "source": [
    "## 훈련/테스트용 데이터 추출"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    remove=('headers', 'footers', 'quotes'),\n",
    "    random_state=156,\n",
    ")\n",
    "X_train = train_dataset.data\n",
    "y_train = train_dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\nWhat I did NOT get with my drive (CD300i) is the System Install CD you\nlisted as #1.  Any ideas about how I can get one?  I bought my IIvx 8/120\nfrom Direct Express in Chicago (no complaints at all -- good price & good\nservice).\n\nBTW, I've heard that the System Install CD can be used to boot the mac;\nhowever, my drive will NOT accept a CD caddy is the machine is off.  How can\nyou boot with it then?\n\n--Dave\n\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "train_dataset.target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = fetch_20newsgroups(\n",
    "    subset='test',\n",
    "    remove=('headers', 'footers', 'quotes'),\n",
    "    random_state=156,\n",
    ")\n",
    "X_test = test_dataset.data\n",
    "y_test = test_dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(11314, 7532)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "def fetch_20newsgroups_train_test_split(random_state=156):\n",
    "    \"\"\"return value is (X_train, X_test, y_train, y_test)\"\"\"\n",
    "    train = fetch_20newsgroups(\n",
    "        subset='train',\n",
    "        remove=('headers', 'footers', 'quotes'),\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    test = fetch_20newsgroups(\n",
    "        subset='test',\n",
    "        remove=('headers', 'footers', 'quotes'),\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    return (train.data, test.data, train.target, test.target)\n",
    "\n",
    "X_train, X_test, y_train, y_test = fetch_20newsgroups_train_test_split(random_state=156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(11314, 7532, 11314, 7532)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "source": [
    "## 피처 벡터화 변환과 머신러닝 모델 학습/예측/평가\n",
    "### Case 1. CountVector"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "cv.fit(X_train)\n",
    "X_train_vector = cv.transform(X_train)\n",
    "X_test_vector = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(11314, 101631)"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "X_train_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6076739245884227"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_vector, y_train)\n",
    "pred = lr.predict(X_test_vector)\n",
    "\n",
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 4 11  1  7  8]\n[ 4 11  6  7  8]\n"
     ]
    }
   ],
   "source": [
    "print(y_test[:5])\n",
    "print(pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer()\n",
    "tv.fit(X_train)\n",
    "X_train_vector = tv.transform(X_train)\n",
    "X_test_vector = tv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(11314, 101631)"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "X_train_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6736590546999469"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_vector, y_train)\n",
    "pred = lr.predict(X_test_vector)\n",
    "\n",
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 4 11  1  7  8]\n[5 1 1 7 8]\n"
     ]
    }
   ],
   "source": [
    "print(y_test[:5])\n",
    "print(pred[:5])"
   ]
  },
  {
   "source": [
    "## 학습에 사용된 파라미터"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.float64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': None, 'min_df': 1, 'ngram_range': (1, 1), 'norm': 'l2', 'preprocessor': None, 'smooth_idf': True, 'stop_words': None, 'strip_accents': None, 'sublinear_tf': False, 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tokenizer': None, 'use_idf': True, 'vocabulary': None}\n"
     ]
    }
   ],
   "source": [
    "print(tv.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "print(lr.get_params())"
   ]
  },
  {
   "source": [
    "### Case 3. stop_words filtering, ngram(1,2), max_df=300"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    ngram_range=(1,2),\n",
    "    max_df=300\n",
    ")\n",
    "tv.fit(X_train)\n",
    "\n",
    "X_train_vec = tv.transform(X_train)\n",
    "X_test_vec = tv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6922464152947424"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_vec, y_train)\n",
    "pred = lr.predict(X_test_vec)\n",
    "\n",
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "source": [
    "### case 4. case 3에서 LogisticRegression C값을 10으로 변경"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr = LogisticRegression(C=10)\n",
    "lr.fit(X_train_vec, y_train)\n",
    "pred = lr.predict(X_test_vec)\n",
    "\n",
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "source": [
    "## Pipeline 과 GridSearchCV를 통한 하이퍼 파라미터 튜닝"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tv', TfidfVectorizer(stop_words='english')),\n",
    "    ('lr', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'tv__ngram_range': [(1,1), (1,2)],\n",
    "    'tv__max_df': [300,700],\n",
    "    'lr__C': [1, 10]\n",
    "}\n",
    "\n",
    "pipe_grid = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "pipe_grid.fit(X_train, y_train)\n",
    "\n",
    "print(pipe_grid.best_params_, pipe_grid.best_score_)"
   ]
  }
 ]
}